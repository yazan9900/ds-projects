{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBp1Fb1lg3EX"
   },
   "source": [
    "## Elements of a NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOMqPcxag3Ec"
   },
   "source": [
    "#### The Natural Language Toolkit **nltk**\n",
    "\n",
    "Import the following libraries by adding the following command in your Jupyter Notebook and run the cell. Feel free to follow along by creating your own Notebook and I have placed a copy in GitHub for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kOcNyqrXg3Ec"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUX9YcrFg3Ed"
   },
   "source": [
    "Next, we download the specific corpus we want to use. Alternatively you can download all the packages using the all parameter. If you are behind a firewall, there is an nltk.set_proxy option available. Check the documentation on nltk.org for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvD7aKhMg3Ed",
    "outputId": "16bef54d-1c5e-47a7-87ef-1d9bcbd31cea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "864vc69rg3Ee"
   },
   "source": [
    "We can use the following command to reference the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BIJS0th9g3Ee"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTQY8OCa1Iqg",
    "outputId": "15a16c28-d203-4590-edbf-def3829e9a01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zed',\n",
       " 'Zedekiah',\n",
       " 'Zeke',\n",
       " 'Zelig',\n",
       " 'Zerk',\n",
       " 'Zeus',\n",
       " 'Zippy',\n",
       " 'Zollie',\n",
       " 'Zolly',\n",
       " 'Zorro']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('male.txt')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0cDrazl1nao",
    "outputId": "168adc97-f3a9-4ef2-b011-f929f9513b85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zonnya',\n",
       " 'Zora',\n",
       " 'Zorah',\n",
       " 'Zorana',\n",
       " 'Zorina',\n",
       " 'Zorine',\n",
       " 'Zsa Zsa',\n",
       " 'Zsazsa',\n",
       " 'Zulema',\n",
       " 'Zuzana']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('female.txt')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nHFri9Mg3Ee"
   },
   "source": [
    "To explore the data available in this corpus, lets run the print command against the two input sources which are male.txt and female.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnbJHhw4g3Ef",
    "outputId": "af390f6a-b06a-4587-9ebb-4f4f24b0f988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Words in male.txt: 2943\n",
      "Count of Words in female.txt: 5001\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of Words in male.txt:\", len(names.words('male.txt')))\n",
    "print(\"Count of Words in female.txt:\", len(names.words('female.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiVHjFUHs0XN",
    "outputId": "f8da71ce-e452-4057-f440-b8d71e06cb6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aamir',\n",
       " 'Aaron',\n",
       " 'Abbey',\n",
       " 'Abbie',\n",
       " 'Abbot',\n",
       " 'Abbott',\n",
       " 'Abby',\n",
       " 'Abdel',\n",
       " 'Abdul',\n",
       " 'Abdulkarim',\n",
       " 'Abdullah',\n",
       " 'Abe',\n",
       " 'Abel',\n",
       " 'Abelard',\n",
       " 'Abner']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('male.txt')[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E33ZMGWHg3Ef"
   },
   "source": [
    "To see a list of the first few words found in each source, lets run the print command against the two input sources which are male.txt and female.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8HvjnRVg3Ef",
    "outputId": "3930ad7d-0762-442d-d136-34293afc485b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample list Male names: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot']\n",
      "Sample list Female names: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample list Male names:\", names.words('male.txt')[0:5])\n",
    "print(\"Sample list Female names:\", names.words('female.txt')[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxpwslkcLly6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmcmD6Psg3Ef"
   },
   "source": [
    "## Create a basic Prediction Output using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSaDGJ0Mg3Eg"
   },
   "source": [
    "Create a gender_features function that returns the last letter of any inputted word. The model will use this classifier feature as input to predict the output which is based on the concept that first names that end the letters a, e and I are more likely to be female and first names ending in k, o, r, s or t are more likely to be male. There will be no output after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5ZtF5Q63g3Eg"
   },
   "outputs": [],
   "source": [
    "def gender_features(word):       \n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZY2mvHvg3Eg"
   },
   "source": [
    "To confirm the function will return a value, enter the following command which prints the last character of any inputted name or word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY9z23Jwg3Eg",
    "outputId": "1b84e241-0086-41c2-8094-58525b3ea0db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'a'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Debra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmcYOHojg3Eh"
   },
   "source": [
    "Create a new variable named labeled_names that loops through both source gender files and assigns a name value pair each name so it can be identified as either male or female for input into the model. To see results after the loop has completed, we print the first few values to validate the labeled_names variable contains data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9K-bSyhEg3Eh",
    "outputId": "1555cb54-4aea-4b43-de77-d23ce948b87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aamir', 'male'), ('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male')]\n"
     ]
    }
   ],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "print(labeled_names[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zyc6cmgg3Eh"
   },
   "source": [
    "Remember, the model should be trained using a random list of values to avoid any bias, therefore, we will input the random function and shuffle all the name and gender combinations which will change the sequence of how they are stored in the labeled_names variable. I added a print so you can see the difference between the output created in the prior step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqD4ECyNg3Eh",
    "outputId": "e97f2aa4-c8a0-46ee-b89d-90b14b2c7f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Carena', 'female'), ('Niven', 'male'), ('Minette', 'female')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "print(labeled_names[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding meaningful features (gender_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0i469OVg3Eh"
   },
   "source": [
    "Next we are going to train the model by creating **features** for each gender using the last letter from each name in the **labeled_names** variable. We print the new variable called featuresets so you can see how the feature will be used in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5XTP2ecg3Ei",
    "outputId": "c9d556db-832b-4be9-c116-a05a24712e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'last_letter': 'a'}, 'female'), ({'last_letter': 'n'}, 'male'), ({'last_letter': 'e'}, 'female')]\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "print(featuresets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrAHrQUkg3Ei"
   },
   "source": [
    "#### Data slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to slice the data from the featureset variable list into two input data sets called train_set and test_set. \n",
    "\n",
    "Once we have those datasets separated, we can use the train_set as an input for the classifier. We use the len() function to give us a sense of the size of each data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RY25CUpag3Ei",
    "outputId": "8823434f-b711-420f-edb9-558da21e2552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of features in Training Set: 7444\n",
      "Count of features in Test Set: 500\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "print(\"Count of features in Training Set:\", len(train_set))\n",
    "print(\"Count of features in Test Set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnqI5W0Lg3Ei"
   },
   "source": [
    "We will now pass the train_set variable as an input to the NLTK Naïve Bayes Classifier. \n",
    "\n",
    "The model is assigned the name classifier so you can call it like a function in the next step. There will be no output once you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mk8jqBIFg3Ej"
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quick validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amZ13iBfg3Ej"
   },
   "source": [
    "We will now validate the results of the model by sending random names into the model using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YpoPmyU6g3Ej",
    "outputId": "688f9923-3d86-4deb-8380-35e35ebdb18b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Sara'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "A215frC32Dag",
    "outputId": "c4fa56f6-2a3c-4805-aa21-d2bc89045410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('tom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tH461nd4g3Ej",
    "outputId": "357d4d8c-bfad-4698-8b6f-3069f544eede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Marc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Mt-TFeGyg3Ej",
    "outputId": "ab4cbfee-04d8-4b2f-ab0f-375adcb784ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Debra'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Yi_Gfv8bg3Ek",
    "outputId": "b74093a7-c74e-4178-e3b1-db59e2e39343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Mohammad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9XBoBJYPg3Ek",
    "outputId": "5f59e1b9-6b5d-4882-b203-2b1f81859196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Ibrahim'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t8T5s13L2Vf"
   },
   "source": [
    "## Stop Words, Count words, Stemmer, Lemmatizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') # Punkt sentence tokenizer\n",
    "nltk.download('brown') # The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University\n",
    "\n",
    "#Check more details in https://www.nltk.org/book/ch02.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3eI7xv4L32_",
    "outputId": "954a1608-9c48-427f-c8b9-3e1656b15b29"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist # Probability model used to encode “frequency distributions”, which count the number of times that each outcome of an experiment occurs.\n",
    "\n",
    "from nltk.stem import PorterStemmer  # Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "from nltk.stem import WordNetLemmatizer # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item\n",
    "\n",
    "my_word_stemmer = PorterStemmer()\n",
    "my_word_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets count brown corpus and check it contains the 1M+ word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all the words found the Brown Corpus = 1,161,192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_of_words = len(brown.words())\n",
    "print('Count of all the words found the Brown Corpus =',format(count_of_words,',d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'figure',\n",
       " 'inside',\n",
       " 'the',\n",
       " 'coral-colored',\n",
       " 'boucle',\n",
       " 'dress',\n",
       " 'was',\n",
       " 'stupefying',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lets use FreqDist and pass the name of the text as an argument. We can inspect the total number of words (\"outcomes\") that have been counted up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = FreqDist(brown.words()) #frequency distributions”, which count the number of times that each outcome of an experiment occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most common words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "* TweetTokenizer\n",
    "* word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a cooool #dummysmiley: :-).', ':-P <3 and some arrows < > -> <-- @remy: This is waaaaayyyy too much for you!!!!!', '!']\n",
      "['This', 'is', 'a', 'cooool', '#', 'dummysmiley', ':', ':', '-', ')', '.', ':', '-P', '<', '3', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--', '@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', '.', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--', '@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "tt = TweetTokenizer()\n",
    "input_data = \"Seth and Becca love the playground.  When it's sunny, they head down there to play.\"\n",
    "tweet = \"This is a cooool #dummysmiley: :-). :-P <3 and some arrows < > -> <-- @remy: This is waaaaayyyy too much for you!!!!!!\"\n",
    "print(sent_tokenize(tweet))\n",
    "print(word_tokenize(tweet))\n",
    "print(tt.tokenize(tweet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer and lematizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming is the process of removing the last few characters of a given word, to obtain a shorter form, even if that form doesn't have any meaning.\n",
    "* Lemmatization on the other hand, is the process of converting the given word into it's base form according to the dictionary meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n",
      "fishing\n"
     ]
    }
   ],
   "source": [
    "# Stemmer \n",
    "print(my_word_stemmer.stem('fishing'))\n",
    "\n",
    "# lemmatizer \n",
    "\n",
    "print(my_word_lemmatizer.lemmatize('fishing'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VA4LgGETMGv9",
    "outputId": "97126fbd-e394-426c-99ff-8cc45a19f861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = The : stem = the : lemma = The\n",
      "word = Fulton : stem = fulton : lemma = Fulton\n",
      "word = County : stem = counti : lemma = County\n",
      "word = Grand : stem = grand : lemma = Grand\n",
      "word = Jury : stem = juri : lemma = Jury\n",
      "word = said : stem = said : lemma = said\n",
      "word = Friday : stem = friday : lemma = Friday\n",
      "word = an : stem = an : lemma = an\n",
      "word = investigation : stem = investig : lemma = investigation\n",
      "word = of : stem = of : lemma = of\n"
     ]
    }
   ],
   "source": [
    "my_list_of_words = brown.words()[:10]\n",
    "\n",
    "for x in my_list_of_words :    \n",
    "    print('word =', x, ': stem =', my_word_stemmer.stem(x), \n",
    "          ': lemma =', my_word_lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2tTt52lg3Ek"
   },
   "source": [
    "## Sentiment analysis in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pre-trained VADER Models for NLTK Sentiment Analysis\n",
    "Check full details on CodeProjects -> https://www.codeproject.com/Articles/5269445/Using-Pre-trained-VADER-Models-for-NLTK-Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUASkZoMg3Ek"
   },
   "source": [
    "Import the NLTK library and download the **vader_lexicon** library so all the functions and features will be available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "N08AZx14g3El"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkE7z3lwg3El",
    "outputId": "8c7e0071-047b-457d-c089-9aff1bd779f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentimentIntensityAnalyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzrWuLZpg3Em"
   },
   "source": [
    "Import the SentimentIntensityAnalyzer from the **NLTK Vader Library** for use in the next steps. There will be no output when you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kMZsWuC4g3Em"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr6CIy-mg3Em"
   },
   "source": [
    "To make it easier, we will assign a variable object named my_ analyser and assign it to the SentimentIntensityAnalyzer() model. There will be no output after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-WzhWntPg3Em"
   },
   "outputs": [],
   "source": [
    "my_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMrtNmbGg3En"
   },
   "source": [
    "Next we will create a variable named **my_input_sentence** and assign it string value of I HATE my school. On the second line we will call the model and pass the variable as an argument to the polarity_scores() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwsSECjAg3En",
    "outputId": "9f25ad33-a7dc-4674-8e51-629f591e737d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.7371, 'neg': 0.0, 'neu': 0.277, 'pos': 0.723}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_sentence = \"I LOVE my school!\"\n",
    "my_analyzer.polarity_scores(my_input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPR9okxvg3En"
   },
   "source": [
    "We are going to import additional libraries to work with and analyze the results so include the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qkRdhtUNg3En"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9NARJTAg3En"
   },
   "source": [
    "We also have to install a new library named twython so include the following command to install it in your Notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gh66oDP0g3En",
    "outputId": "a2be4fc7-dff7-4eff-e53d-45d6b0201979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twython\n",
      "  Downloading twython-3.8.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.4.0 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from twython) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from twython) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2019.6.16)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sphinx/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
      "Installing collected packages: twython\n",
      "Successfully installed twython-3.8.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install twython #Actively maintained, pure Python wrapper for the Twitter API. Supports both normal and streaming Twitter APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTT6-Lhwg3Eo"
   },
   "source": [
    "If required, re-import the NLTK library and import the SentimentIntensityAnalyzer module. No output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "LEMc8wCbg3Eo"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tP5D90Tg3Eo"
   },
   "source": [
    "Define a variable as analyzer to easier reference later in the code. No output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7OWpla_3g3Ep"
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCRp4mE4g3Ep"
   },
   "source": [
    "If required, re-download the NLTK vader_lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Y3LfeXHg3Ep",
    "outputId": "07739e36-e941-42c0-f22f-ac9e0b973de7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sphinx/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNIwZtp4g3Ep"
   },
   "source": [
    "Now we will read in the csv file using the pandas library and assign the result to a variable named sentences. To validate the results, you run the len() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZpaAxVzg3Ep",
    "outputId": "ad7b3fc1-63ce-459d-85c9-b08195fb9166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL='https://drive.google.com/uc?export=download&id=1OVnNj9IFF9fS5dJSRFYsqeo2C3vC8yUx'\n",
    "sentences = pd.read_csv(URL)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AkAIareg3Eq"
   },
   "source": [
    "To preview the data and validate your DataFrame is loaded correctly, you can run a head() command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "AyS-MgeLg3Eq",
    "outputId": "7121d00d-8f02-4d11-88dd-d00bf2217f22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I Hate my School!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@socialmediahandle I learned something new today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I need to take a cool trip to Austraila!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The restaurant service was amazing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I will never go back there again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>You learn something new every day - this place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>First Impressions - this is good but then it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A bit pricey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Love them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>meh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   1                                I Hate my School!!!\n",
       "1   2   @socialmediahandle I learned something new today\n",
       "2   3           I need to take a cool trip to Austraila!\n",
       "3   4                The restaurant service was amazing!\n",
       "4   5                  I will never go back there again!\n",
       "5   6  You learn something new every day - this place...\n",
       "6   7  First Impressions - this is good but then it w...\n",
       "7   8                                       A bit pricey\n",
       "8   9                                          Love them\n",
       "9  10                                                meh"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87uAI2Hng3Eq"
   },
   "source": [
    "The next block of code includes a few steps which \n",
    "* look through the DataFrame, \n",
    "* analyze the text source, \n",
    "* apply the VADER Sentiment metrics\n",
    "* assign the results to a numpy array to easier usage. \n",
    "\n",
    "No output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "Sq8qc4w8g3Eq"
   },
   "outputs": [],
   "source": [
    "i=0 #reset counter for loop\n",
    "\n",
    "#initialize variables\n",
    "my_vader_score_compound = [ ] \n",
    "my_vader_score_positive = [ ]  \n",
    "my_vader_score_negative = [ ] \n",
    "my_vader_score_neutral = [ ] \n",
    "\n",
    "while (i<len(sentences)):\n",
    "\n",
    "    my_analyzer = analyzer.polarity_scores(sentences.iloc[i]['text'])\n",
    "    my_vader_score_compound.append(my_analyzer['compound'])\n",
    "    my_vader_score_positive.append(my_analyzer['pos'])\n",
    "    my_vader_score_negative.append(my_analyzer['neg'])    \n",
    "    my_vader_score_neutral.append(my_analyzer['neu']) \n",
    "    \n",
    "    i = i+1\n",
    "    \n",
    "#converting sentiment values to numpy for easier usage\n",
    "my_vader_score_compound = np.array(my_vader_score_compound)\n",
    "my_vader_score_positive = np.array(my_vader_score_positive)\n",
    "my_vader_score_negative = np.array(my_vader_score_negative)\n",
    "my_vader_score_neutral = np.array(my_vader_score_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvChTkQlg3Eq"
   },
   "source": [
    "Now we can extend the source DataFrame to include the results from the VADER Sentiment model. This will create four new columns and no output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "znveeHYQg3Er"
   },
   "outputs": [],
   "source": [
    "sentences['my VADER Score'] = my_vader_score_compound\n",
    "sentences['my VADER score - positive'] = my_vader_score_positive\n",
    "sentences['my VADER score - negative'] = my_vader_score_negative\n",
    "sentences['my VADER score - neutral'] = my_vader_score_neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5ItwKVYg3Er"
   },
   "source": [
    "To see the changes, we run the head() function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "gEuNpWFYg3Er",
    "outputId": "72f943c3-5b72-418d-9a1a-33a0a681bcda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>my VADER Score</th>\n",
       "      <th>my VADER score - positive</th>\n",
       "      <th>my VADER score - negative</th>\n",
       "      <th>my VADER score - neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I Hate my School!!!</td>\n",
       "      <td>-0.6784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@socialmediahandle I learned something new today</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I need to take a cool trip to Austraila!</td>\n",
       "      <td>0.3802</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The restaurant service was amazing!</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I will never go back there again!</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>You learn something new every day - this place...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>First Impressions - this is good but then it w...</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A bit pricey</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Love them</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>meh</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  my VADER Score  \\\n",
       "0   1                                I Hate my School!!!         -0.6784   \n",
       "1   2   @socialmediahandle I learned something new today          0.0000   \n",
       "2   3           I need to take a cool trip to Austraila!          0.3802   \n",
       "3   4                The restaurant service was amazing!          0.6239   \n",
       "4   5                  I will never go back there again!          0.0000   \n",
       "5   6  You learn something new every day - this place...          0.6249   \n",
       "6   7  First Impressions - this is good but then it w...          0.3400   \n",
       "7   8                                       A bit pricey          0.0000   \n",
       "8   9                                          Love them          0.6369   \n",
       "9  10                                                meh         -0.0772   \n",
       "\n",
       "   my VADER score - positive  my VADER score - negative  \\\n",
       "0                      0.000                      0.696   \n",
       "1                      0.000                      0.000   \n",
       "2                      0.302                      0.000   \n",
       "3                      0.506                      0.000   \n",
       "4                      0.000                      0.000   \n",
       "5                      0.313                      0.000   \n",
       "6                      0.254                      0.000   \n",
       "7                      0.000                      0.000   \n",
       "8                      0.808                      0.000   \n",
       "9                      0.000                      1.000   \n",
       "\n",
       "   my VADER score - neutral  \n",
       "0                     0.304  \n",
       "1                     1.000  \n",
       "2                     0.698  \n",
       "3                     0.494  \n",
       "4                     1.000  \n",
       "5                     0.687  \n",
       "6                     0.746  \n",
       "7                     1.000  \n",
       "8                     0.192  \n",
       "9                     0.000  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8PF9QKfg3Er"
   },
   "source": [
    "While this information is useful, it still requires the user to scan through the results row by row. Let’s make it easier to analyze and summarize the results by creating a new column which categorizes the compound score results. No output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gVjwBpN-g3Er"
   },
   "outputs": [],
   "source": [
    "i=0 #reset counter for loop\n",
    "\n",
    "#initialize variables\n",
    "my_prediction = [ ] \n",
    "\n",
    "while (i<len(sentences)):\n",
    "    if ((sentences.iloc[i]['my VADER Score'] >= 0.3)):\n",
    "        my_prediction.append('positive')\n",
    "    elif ((sentences.iloc[i]['my VADER Score'] >= 0) & (sentences.iloc[i]['my VADER Score'] < 0.3)):\n",
    "        my_prediction.append('neutral')\n",
    "    elif ((sentences.iloc[i]['my VADER Score'] < 0)):\n",
    "        my_prediction.append('negative')     \n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LxTVoZng3Es"
   },
   "source": [
    "Similar to before, we will take the results and add a new column to our DataFrame called my prediction sentiment. No output will be displayed after you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "HgTI1P4Ig3Es"
   },
   "outputs": [],
   "source": [
    "sentences['my predicted sentiment'] = my_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0WiCb6Wg3Es"
   },
   "source": [
    "To see the changes, we run the head() function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "syuFzjaJg3Es",
    "outputId": "3e892e08-edb8-4642-b98f-5ea1f7510e2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>my VADER Score</th>\n",
       "      <th>my VADER score - positive</th>\n",
       "      <th>my VADER score - negative</th>\n",
       "      <th>my VADER score - neutral</th>\n",
       "      <th>my predicted sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I Hate my School!!!</td>\n",
       "      <td>-0.6784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.304</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@socialmediahandle I learned something new today</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I need to take a cool trip to Austraila!</td>\n",
       "      <td>0.3802</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.698</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The restaurant service was amazing!</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I will never go back there again!</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>You learn something new every day - this place...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.687</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>First Impressions - this is good but then it w...</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.746</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A bit pricey</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Love them</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>meh</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ... my predicted sentiment\n",
       "0   1  ...               negative\n",
       "1   2  ...                neutral\n",
       "2   3  ...               positive\n",
       "3   4  ...               positive\n",
       "4   5  ...                neutral\n",
       "5   6  ...               positive\n",
       "6   7  ...               positive\n",
       "7   8  ...                neutral\n",
       "8   9  ...               positive\n",
       "9  10  ...               negative\n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.groupby('my predicted sentiment').size().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfc1zrVLg3Es"
   },
   "source": [
    "To make it easier to interpret the results, lets create a data visualization against the DataFrame by summarizing the results using an aggregate group by. We use the plot() function from the matplotlib library to display a horizontal bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "w7ERsd-9g3Es",
    "outputId": "59015d3c-45b8-4839-f7b7-b801e4b07283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences.groupby('my predicted sentiment').size().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLi2772VMof4"
   },
   "source": [
    "#### Keywords and Libraries \n",
    "* Natural Language Toolkit (NLTK)\n",
    "* TextBlob\n",
    "* CoreNLP\n",
    "* Gensim\n",
    "* spaCy\n",
    "* polyglot\n",
    "* scikit–learn\n",
    "* Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project example: https://sunscrapers.com/blog/8-best-python-natural-language-processing-nlp-libraries/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- https://github.com/CullenBaker/BERT-Emotion-Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- https://github.com/NBrisbon/Silmarillion-NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- https://github.com/ajayshewale/Sentiment-Analysis-of-Text-Data-Tweets-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SenitementAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
